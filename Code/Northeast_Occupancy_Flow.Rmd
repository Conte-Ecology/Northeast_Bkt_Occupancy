11-01-2013 NHDPlus SMALL catchments and brook trout presence/absence in Northeast 

========================================================

## Set working directory & library
```{r working directory and libraries, warning=FALSE, message=FALSE, echo=TRUE, results='hide'}
setwd('/Users/Dan/Documents/Research/Stream_Climate_Change/Brook_Trout/Occupancy/Northeast_Bkt_Occupancy/')
#setwd('C:/Users/dhocking/Documents/Northeast_Bkt_Occupancy/')

getwd()
# library(reshape2)
library(rjags)
library(ggplot2)
#library(plyr)
library(dplyr)
library(data.table)
library(lme4)
library(arm) # used for sim function
library(knitr)
library(foreign) # read.dbf file
#library(nnet)
#library(ncf)
#library(fda)
library(boot) # needed for inv.logit function for AUC
library(AUC)
#library(ROCR)
#library(nlme)
#load.module("glm")
```

```{r global setting, eval=TRUE, echo=FALSE}
runInRStudio <- FALSE
if(!runInRStudio) opts_chunk$set( warning=FALSE,message=FALSE,fig.width=10, fig.height=7, comment=NA )
```

## Read in data
```{r read in data, results='hide'}
# Environmental data
load("NENY_CovariateData_2014-03-13.RData") # Loads "LocalStats" and "UpstreamStats"
load("flow_pred_mean_CTR_03_2014.RData") # Loads df "pred" which can be joined by FEATUREID

temp.stream <- fread("BP_Predictions2010_CTDEP_MAFW_MAUSGS_NHFG_NHDES_USFS_VTFWS.csv", header = TRUE, sep = ",")

#temp.stream <- readRDS("derived_site_metrics.RData")

catchments <- read.dbf("catch_lower_ctr.dbf")
```

## Combine Data
```{r merge data, results='hide'}
# Use upstream stats
data <- merge(UpstreamStats, LocalStats[ , c("FEATUREID", "PresenceAbsence")], by = "FEATUREID") # get P/A from local
data <- merge(data, pred[ , c("FEATUREID", "pred_flow")], by = "FEATUREID") # get flow
#temp.stream$FEATUREID <- temp.stream$featureid
data <- merge(data, temp.stream, by = c("FEATUREID"))

data$FEATUREID <- as.character(data$FEATUREID)
# data$pres <- as.factor(as.character(data$PresenceAbsence))

data$OnChannelWaterSqKM <- data$ImpoundmentsOpenSqKM
data$OnChannelWetlandSqKM <- data$ImpoundmentsAllSqKM - data$ImpoundmentsOpenSqKM
data$OffChannelWaterSqKM <- data$OffChannelOpenSqKM
data$OffChannelWetlandSqKM <- data$OffChannelAllSqKM - data$OffChannelOpenSqKM

data.all <- data

# clip to catchments with good/consistent NHDplus catchment delineations
data <- data.all[data.all$FEATUREID %in% catchments$FEATUREID, ]
```

## remove catchments without fish surveys
```{r remove cat without fish}
data2 <- subset(data, !is.na(PresenceAbsence)) # 3719 of 27,784 catchments

#data2 <- subset(data2, state != 'Vermont')  # remove a few labled as Vermont
```

## Scatterplot Matrices
```{r Scatterplot Matrices}
## put histograms on the diagonal
panel.hist <- function(x, ...) {
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(usr[1:2], 0, 1.5) )
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks; nB <- length(breaks)
  y <- h$counts; y <- y/max(y)
  rect(breaks[-nB], 0, breaks[-1], y, col="gray", ...)
  }

panel.cor <- function(x, y, method = "pearson", use = "pairwise.complete.obs", digits = 2, prefix="", cex.cor) {
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  x = x
  y = y
  Cor <- switch(method,
                spearman = cor(x, y, method = "spearman", use = use),
                pearson = cor(x, y, method = "pearson", use = use),
                kendell = cor(x, y, method = "kendall", use = use),
                stop("\nThe type of correlation must be spearman, pearson, or kendall\n"))
  txt <- format(c(Cor, 0.123456789), digits = digits)[1]
  txt <- paste(prefix, txt, sep="")
  #if(missing(cex.cor)) cex <- 0.9/strwidth(txt)
  text(0.5, 0.5, txt, cex = 1.5)
  #text(0.9, 0.9, Cor.Type, cex = 1)
}

panel.smooth.big <- function (x, y, col = par("col"), bg = NA, pch = par("pch"), 
    cex = 1, col.smooth = "red", span = 2/3, iter = 3, ...) {
  x <- sample(x = x, size=1000, replace = F)
  y <- sample(x = y, size = 1000, replace = F)
    #x <- rnorm(500, mean(x, na.rm = T), sd(x, na.rm = T)) # alpha bleeding
    #y <- rnorm(500, mean(y, na.rm = T), sd(y, na.rm = T)) # alpha bleeding
    points(x, y, pch = pch, col = col, bg = bg, cex = cex)
    ok <- is.finite(x) & is.finite(y)
    if (any(ok)) 
        lines(stats::lowess(x[ok], y[ok], f = span, iter = iter), 
            col = col.smooth, ...)
}

Precip <- data2[ , c("PresenceAbsence", "pred_flow", "AnnualTmaxC", "AnnualTminC", "AnnualPrcpMM", "WinterPrcpMM", "SummerPrcpMM", "AtmDepositionNO3", "AtmDepositionSO4")]
pairs(Precip, upper.panel=panel.smooth, lower.panel=panel.cor, diag.panel=panel.hist)

Landuse <- data2[ , c("PresenceAbsence", "CalculatedTotDASqKM", "Forest", "Herbacious", "Agriculture", "Developed", "WetlandOrWater", "Water", "Impervious")]
pairs(Landuse, upper.panel=panel.smooth, lower.panel=panel.cor, diag.panel=panel.hist)

Topography <- data2[ , c("PresenceAbsence", "BasinSlopeDEG", "DrainageClass", "ReachElevationM", "StreamOrder", "AnnualTmaxC", "AnnualTminC")]
pairs(Topography, upper.panel=panel.smooth, lower.panel=panel.cor, diag.panel=panel.hist)

Geologic <- data2[ , c("PresenceAbsence", "HydrologicGroupA", "HydrologicGroupAB", "HydrologicGroupCD", "HydrologicGroupD4", "HydrologicGroupD1", "SurficialCoarseA", "SurficialCoarseB", "SurficialCoarseC", "SurficialCoarseD", "PercentSandy")]
pairs(Geologic, upper.panel=panel.smooth, lower.panel=panel.cor, diag.panel=panel.hist)
```


## Remove unneeded variables and rename remaining
```{r remove extra variables and rename}
data2 <- data2[ , c("PresenceAbsence", "TotDASqKM", "ReachElevationM", "ReachSlopeDEG", "Forest", "Agriculture", "CONUSWetland", "CONUSOpenWater", "Developed", "AnnualTmaxC", "AnnualTminC", "WinterPrcpMM", "SummerPrcpMM", "AnnualPrcpMM", "SurficialCoarseC", "HydrologicGroupAB", "StreamOrder", "pred_flow", "SummerMaxT", "RiseSlope", "FallSlope", "TNC_DamCount","Latitude", "Longitude", "HUC_8", "HUC_12", "FEATUREID")] 

names(data2) <- c("pres", "area", "elev", "slope", "forest", "ag", "wetland", "water", "develop", "tmax", "tmin", "precip.winter", "precip.summer", "precip", "surfC", "hydroAB", "stream.order", "flow", "tmax.stream", "rise.slope", "fall.slope", "dams", "lat", "lon", "HUC_8", "HUC_12", "FEATUREID")
```

## also remove catchments without HUC10 ID (weird)
```{r remove catchment w/o HUC10 ID}
data2$HUC_10 <- substr(data2$HUC_12, 1, 10)
data2 <- subset(data2, !is.na(HUC_10))
```

## Add state to the dataframe and visualize distribution of points
```{r add state}
library(sp)
library(maps)
library(maptools)

# The single argument to this function, pointsDF, is a data.frame in which:
#   - column 1 contains the longitude in degrees (negative in the US)
#   - column 2 contains the latitude in degrees

latlong2state <- function(pointsDF) {
  library(sp)
  library(maps)
  library(maptools)
    # Prepare SpatialPolygons object with one SpatialPolygon
    # per state (plus DC, minus HI & AK)
    states <- map('state', fill=TRUE, col="transparent", plot=FALSE)
    IDs <- sapply(strsplit(states$names, ":"), function(x) x[1])
    states_sp <- map2SpatialPolygons(states, IDs=IDs,
                     proj4string=CRS("+proj=longlat +datum=wgs84"))

    # Convert pointsDF to a SpatialPoints object 
    pointsSP <- SpatialPoints(pointsDF, 
                    proj4string=CRS("+proj=longlat +datum=wgs84"))

    # Use 'over' to get _indices_ of the Polygons object containing each point 
    indices <- over(pointsSP, states_sp)

    # Return the state names of the Polygons object containing each point
    stateNames <- sapply(states_sp@polygons, function(x) x@ID)
    stateNames[indices]
}

# Test the function using points in Wisconsin and Oregon.
testPoints <- data.frame(x = c(-90, -120), y = c(44, 44))

latlong2state(testPoints)

data2$state <- latlong2state(data2[ , c("lon", "lat")]) # not sure if this works perfectly with the datum or lat long of drainages (falls over border)
length(data2[data2$state == "pennsylvania", ]$state)
length(data2[data2$state == "new jersey", ]$state)
length(data2[data2$state == "vermont", ]$state)
length(data2[data2$state == "new york", ]$state)
length(data2[data2$state == "rhode island", ]$state)
length(data2[data2$state == "maine", ]$state)

# Map - one of these packages below causes problems with ggplots
#crs.geo <- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")  # geographical, datum WGS84
#locs <- data2[ , c("lon", "lat")]
#coordinates(locs) <- locs
#proj4string(locs) <- crs.geo  # define projection system of our data
#summary(locs)

#locs.50 <- data2[data2$area > 50 & data$area <= 200 , c("lon", "lat")]
#coordinates(locs.50) <- locs.50
#proj4string(locs.50) <- crs.geo

#locs.200 <- data2[data2$area > 200 , c("lon", "lat")]
#coordinates(locs.200) <- locs.200
#proj4string(locs.200) <- crs.geo

#library(dismo)
#nemap <- gmap(locs, type = "satellite")
#locs.ne.merc <- Mercator(locs)  # Google Maps are in Mercator projection.
#locs.ne.merc.50 <- Mercator(locs.50)
#locs.ne.merc.200 <- Mercator(locs.200)
# This function projects the points to that projection to enable mapping
#plot(nemap)
#points(locs.ne.merc, pch = 20, col = "black", cex = 0.1)
#points(locs.ne.merc.50, pch = 20, col = "orange", cex = 0.11)
#points(locs.ne.merc.200, pch = 20, col = "red", cex = 0.111)
#legend("topleft", legend=c("1-50 km2", "50-200 km2", "200+ km2"), pch=c(20, 20, 20), col=c("black", "orange", "red"))

```

## Watershed size & brook trout occurrence
### brook trout typically occurs in small watersheds
```{r watershed size & wbk}
# deciding upper range of watershed size for occupance analysis, to remove large watersheds (i.e. outliers)

hist(data2[ data2$pres == 1, ]$area) # a few streams don't have the right size because they aren't on NHDplus and are small tribs directly into main river so they get the entire river basin area
data3 <- data2[data2$area <= 500, ]

ggplot(data = data3, aes(x = area)) + geom_histogram(aes(area), binwidth = 5) + facet_wrap( ~ pres)

# Remove rising and falling slope outliers
summary(data3$rise.slope)
boxplot(data3$rise.slope)
data3 <- data3[which(data3$rise.slope > 0), ]
data3 <- data3[which(data3$fall.slope > 0), ]
final.pairs <- data3[ , c("pres", "area", "slope", "forest", "wetland", "rise.slope", "fall.slope", "tmax.stream", "tmax", "tmin", "precip", "surfC", "hydroAB", "flow")]
pairs(final.pairs, upper.panel=panel.smooth, lower.panel=panel.cor, diag.panel=panel.hist)
```

## prepare df for fitting models
```{r df fit}
# reorder columns
data.fit <- data3[ , c("FEATUREID","area","precip", "precip.summer", "tmax.stream", "rise.slope", "fall.slope", "tmax", "tmin", "surfC", "forest", "ag", "slope","pres", "hydroAB", "wetland", "water", "elev", "develop", "dams", "flow", "state", "HUC_8","HUC_10")] # ,"state","stateID"

#data.fit <- data3
```

## Clean up unusual & outliers
```{r clean up outliers}
summary(data.fit)

# catchment area: remove those < 1km2 (errors? - I don't think so)
#data.fit2 <- subset(data.fit, area >= 1)
data.fit2 <- subset(data.fit, is.na(slope) == FALSE)
data.fit2 <- subset(data.fit2, is.na(elev) == FALSE)
data.fit2 <- subset(data.fit2, is.na(tmax.stream) == FALSE)

summary(data.fit2) # no NA remaining
```

## Create unique ID for HUC 10 starting with 1
This is useful for plotting so labels fit
```{r create unique ID for HUC 10}
AllHuc10 <- subset(data.fit2, !duplicated(HUC_10, fromLast=FALSE))
AllHuc10$HUC_10n <- as.numeric(AllHuc10$HUC_10) # just to order below. to be deleted
AllHuc10 <- AllHuc10[ order(AllHuc10$HUC_10n), ]
AllHuc10 <- data.frame(cbind(AllHuc10$HUC_10, c(1:nrow(AllHuc10))))
names(AllHuc10) <- c("HUC_10","fHuc10")
str(AllHuc10)
AllHuc10$HUC_10 <- as.character(AllHuc10$HUC_10)

## add HUC10 ID to fit df
data.fit2 <- merge(x=data.fit2, y=AllHuc10, by="HUC_10")
```

If animals are responding to temperature or precip at the HUC level, it could get lost in the catchment-level temperatures. For example, a catchment might be warm and occupied because the HUC-level stream is cool and there are cool patches even in the warm catchment, so the catchment is occupied because the whole area is cool on average. They may move out of the warm areas duing the hottest periods but it is at least occupied some of the time because the whole area is cool on average and therefore there are cool refugia. Therefore, it might make scense to have a HUC-level temp and precip in addition to the catchment-level, otherwise the random effect sucks up that variation without explaination or predictive power.

```{r HUC-level mean temp and precip}
# take a weighted average by HUC
data.fit2$huc10 <- as.numeric(data.fit2$fHuc10)
bar <- NA
for(i in 1:max(data.fit2$huc10)) {
  foo <- subset(data.fit2, subset = huc10 == i)
  bar[i] <- sum(foo$area * foo$tmax) / sum(foo$area)
}

df.huc <- data.frame(1:max(data.fit2$huc10), bar)
names(df.huc) <- c("huc10", "tmax.huc")

data.fit2 <- merge(data.fit2, df.huc, by = "huc10")

```

## Standardize covariates

```{r remove outliers and standardize or transform covariates}
# check cov

par(mfrow = c(1, 2))
boxplot(data.fit2$precip) ; hist(data.fit2$precip)  # looks good
boxplot(data.fit2$area); hist(data.fit2$area)    # needs transformation
boxplot(log(data.fit2$area + 1)); hist(log(data.fit2$area + 1)) # log transformation works well for area
boxplot(data.fit2$tmax)   ; hist(data.fit2$tmax)    # maybe ok a bit of a tail on lower side
boxplot(sqrt(18 - data.fit2$tmax))   ; hist(sqrt(18 - data.fit2$tmax)) # improvement probably not worth it
boxplot(data.fit2$tmin)   ; hist(data.fit2$tmin)    # looks good
boxplot(data.fit2$forest) ; hist(data.fit2$forest)  # needs transformation
boxplot(asin(data.fit2$forest/100)) ; hist(asin(data.fit2$forest/100))  #arc-sine transformation works well for forest
boxplot(data.fit2$slope); hist(data.fit2$slope) # needs transformation
boxplot(log(data.fit2$slope + 1))  ; hist(log(data.fit2$slope + 1))   #square-root transformation works okay for slope
boxplot(data.fit2$surfC); hist(data.fit2$surfC) # needs transformation
boxplot(log(data.fit2$surfC + 1)); hist(log(data.fit2$surfC + 1)) # not perfect but useable
boxplot(data.fit2$wetland/100); hist(data.fit2$wetland/100) # needs transformation
boxplot(log(data.fit2$wetland + 1)); hist(log(data.fit2$wetland + 1)) # log works well
boxplot(data.fit2$water); hist(data.fit2$water) # needs transformation
boxplot(log(data.fit2$water + 1)); hist(log(data.fit$water + 1)) # still problematic but maybe useable
boxplot(sqrt(data.fit2$water + 0.5)); hist(sqrt(data.fit2$water + 0.5)) # no better
boxplot(asin(data.fit2$water/100)); hist(asin(data.fit2$water/100)) # no luck
boxplot((data.fit2$water + 0.5)^-1); hist((data.fit2$water + 0.5)^-1) # no
boxplot(data.fit2$ag); hist(data.fit2$ag) # needs transformation
boxplot(log(data.fit2$ag + 1)); hist(log(data.fit2$ag + 1)) # log works well
boxplot(data.fit2$precip.summer); hist(data.fit2$precip.summer) # maybe okay
boxplot(data.fit2$dams); hist(data.fit2$dams) # needs transformation
boxplot(sqrt(data.fit2$dams + 0.5)); hist(sqrt(data.fit2$dams + 0.5)) # still not good
boxplot(log(data.fit2$dams + 1)); hist(log(data.fit2$dams + 1)) # nope
boxplot(data.fit2$elev); hist(data.fit2$elev) # maybe ok - try trans
boxplot(sqrt(data.fit2$elev + 0.5)); hist(sqrt(data.fit2$elev + 0.5)) # sqrt works great for elev
boxplot(data.fit2$hydroAB); hist(data.fit2$hydroAB) # looks okay
boxplot(data.fit2$flow); hist(data.fit2$flow) # needs transformation
boxplot(log(data.fit2$flow + 1)); hist(log(data.fit2$flow + 1)) # log trans works well for flow
boxplot(data.fit2$develop); hist(data.fit2$develop) # needs transformation
boxplot(log(data.fit2$develop + 1)); hist(log(data.fit2$develop + 1)) # log trans works great for develop
boxplot(data.fit2$tmax.stream); hist(data.fit2$tmax.stream) # looks decent
boxplot(log(data.fit2$tmax.stream + 1)); hist(log(data.fit2$tmax.stream + 1)) # no better but consider Gelman and Hill p548 about multiplicative error and log transforming all positive variables
boxplot(data.fit2$rise.slope); hist(data.fit2$rise.slope) # fine
boxplot(log(data.fit2$rise.slope + 1)); hist(log(data.fit2$rise.slope + 1)) # no better
par(mfrow = c(1,1))

# Transform Variables
data.fit3 <- data.fit2
data.fit3$log.area <- log(data.fit3$area + 1)
data.fit3$asin.forest <- asin(data.fit3$forest/100)
data.fit3$log.slope <- log(data.fit3$slope + 1)
data.fit3$log.surfC <- log(data.fit3$surfC + 1)
data.fit3$log.wet <- log(data.fit3$wetland + 1)
data.fit3$log.ag <- log(data.fit3$ag + 1)
data.fit3$sqrt.elev <- sqrt(data.fit3$elev + 0.5)
data.fit3$log.flow <- log(data.fit3$flow + 1)
data.fit3$log.develop <- log(data.fit3$develop + 1)
data.fit3$log.water <- log(data.fit3$water + 1)
data.fit3$log.dams <- log(data.fit3$dams + 1)

# re-order columns
data.fit3 <- data.fit3[c("pres", "tmax.huc", "log.area", "precip", "tmax.stream", "rise.slope", "fall.slope", "tmax", "tmin", "asin.forest", "log.slope", "log.surfC", "log.ag", "precip.summer", "sqrt.elev", "hydroAB", "log.develop", "log.flow", "log.wet", "log.water", "FEATUREID", "state", "fHuc10", "HUC_8", "HUC_10", "log.dams")]

pairs(data.fit3[ , c(1:19, 25)], upper.panel=panel.smooth, lower.panel=panel.cor, diag.panel=panel.hist)

data.fit2 <- data.fit2[c("pres", "tmax.huc", "area", "precip", "tmax.stream", "rise.slope", "fall.slope", "tmax", "tmin", "forest", "slope", "surfC", "ag", "precip.summer", "elev", "hydroAB", "develop", "flow", "wetland", "water", "FEATUREID", "state", "fHuc10", "HUC_8", "HUC_10", "dams")]

# Standardize continuous covariates for analysis instead of using transformations
# Zuur p. 485
data.fit.std <- cbind(data.fit2[ ,c(1, 21:26)],
                      apply(X = data.fit2[ ,2:20], MARGIN=2,
                            FUN = function(x){(x-mean(x, na.rm=TRUE)) / sd(x, na.rm=TRUE)}))

summary(data.fit.std) # didn't standardize # of dams but this could be considered roughly continuous
```

## Pair plot of environmental covariates
```{r pair plot of env covariates}
# function for pearson correlation (http://www2.warwick.ac.uk/fac/sci/moac/people/students/peter_cock/r/iris_plots/)

# pair plot
pairs(data.fit.std[ ,c(1, 8:22)], main = "Pairs plot of standardized covariates", upper.panel=panel.smooth, lower.panel=panel.cor, diag.panel=panel.hist)

# summer precip isn't overly correlated with any 1 variable but moderately correlated with multiple (forest, temp, precip) so probably unnecessary to use in the models

# consider whether want a quadratic effect of area or if the data is just sparse when have small drainages
par(mfrow = c(2, 1))
plot(data.fit2$area, data.fit2$pres)
lines(smooth.spline(data.fit2$area, data.fit2$pres), col = "red")
hist(data.fit2$area)
par(mfrow = c(1, 1))

# probably no need to have quadratic area effect. Area doesn't seem to make a difference.

# elevation is the only variable that can't include b/c of colinarity and can only use tmin or tmax
# might not be able to use slope because moderate collinarity with multiple variables
# also probably just use rising slope of air-water relationship. Rising and falling are correlated but rising has a bit more variability

```

## Validation Setup
```{r separate data for fitting and for validation}
# How many HUC 8, 10, and 12 for leaving out as validation set
length(unique(data.fit.std$HUC_8)) 
length(unique(data.fit.std$HUC_10)) # number of HUC10 
length(data.fit.std$HUC_10) # number total catchments

p.valid <- 0.1 # Percent of data to retain for validation purposes
n.fit <- floor(length(unique(data.fit.std$HUC_10)) * (1 - p.valid))

set.seed(2346)
huc.fit <- sample(unique(data.fit.std$HUC_10), n.fit, replace = FALSE) # select HUCs to hold back for testing 
df.valid <- subset(data.fit.std, !HUC_10 %in% huc.fit) # 10% data for validation
df.fit <- subset(data.fit.std, HUC_10 %in% huc.fit)    # 90% data for model fitting

n.fit # 138 of 154 HUCs used for model fitting (16 retained for validation)
dim(df.fit)[1] # 3239 of 3645 catchments samples used for fitting

```

## Data for cross validation (use entire dataset for initial models)
```{r results = 'hide'}
# df.fit <- data.fit.std

```

            ################################################
            ## Model fitting of catchments < 50 km2 ##
            ################################################

## GLMM

## Global model with all covariates and check the optimizer:
(http://stackoverflow.com/questions/21344555/convergence-error-for-development-version-of-lme4)[http://stackoverflow.com/questions/21344555/convergence-error-for-development-version-of-lme4]

```{r glmm optimizer test, cache=TRUE, results='hide'}
# varying intercept: all covariates
g0.bobyqa <- glmer(pres ~ area + precip + tmax + forest + slope + surfC + dams + hydroAB + (1|fHuc10), family = binomial(link = "logit"), data = df.fit, control=glmerControl(optimizer="bobyqa", check.conv.singular="warning")) # no warning
summary(g0.bobyqa)

g0.NM <- update(g0.bobyqa,control=glmerControl(optimizer="Nelder_Mead")) # warning
library(optimx)
g0.nlminb <- update(g0.bobyqa,control=glmerControl(optimizer="optimx",
                              optCtrl=list(method="nlminb"))) # no warning
g0.LBFGSB <- update(g0.bobyqa,control=glmerControl(optimizer="optimx",
                              optCtrl=list(method="L-BFGS-B"))) # warning

library(nloptr)
## from https://github.com/lme4/lme4/issues/98:
defaultControl <- list(algorithm="NLOPT_LN_BOBYQA",xtol_rel=1e-6,maxeval=1e5)
nloptwrap2 <- function(fn,par,lower,upper,control=list(),...) {
    for (n in names(defaultControl)) 
      if (is.null(control[[n]])) control[[n]] <- defaultControl[[n]]
    res <- nloptr(x0=par,eval_f=fn,lb=lower,ub=upper,opts=control,...)
    with(res,list(par=solution,
                  fval=objective,
                  feval=iterations,
                  conv=if (status>0) 0 else status,
                  message=message))
}
g0.bobyqa2 <- update(g0.bobyqa,control=glmerControl(optimizer=nloptwrap2)) #no warning
g0.NM2 <- update(g0.bobyqa,control=glmerControl(optimizer=nloptwrap2,
                           optCtrl=list(algorithm="NLOPT_LN_NELDERMEAD"))) # no warning

getpar <- function(x) c(getME(x,c("theta")),fixef(x))
modList <- list(bobyqa=g0.bobyqa,NM=g0.NM,nlminb=g0.nlminb,
                bobyqa2=g0.bobyqa2,NM2=g0.NM2,LBFGSB=g0.LBFGSB)
ctab <- sapply(modList,getpar)
library(reshape2)
mtab <- melt(ctab)
library(ggplot2)
theme_set(theme_bw())
ggplot(mtab,aes(x=Var2,y=value,colour=Var2))+
    geom_point()+facet_wrap(~Var1,scale="free")

ggplot(subset(mtab,Var2 %in% c("NM", "LBFGSB", "NM2","bobyqa","bobyqa2")),
       aes(x=Var2,y=value,colour=Var2))+
    geom_point()+facet_wrap(~Var1,scale="free")
```
Use the built-in bobyqa optimizer

## GLMM modeling
Global model and random effects selection
```{r glmm comparison 1, cache=TRUE, results='hide'}
# Global model wih varying intercept
glmm.M01 <- glmer(pres ~ flow  + tmax.stream + rise.slope + forest + wetland + surfC + dams + hydroAB     + tmax.stream * forest + rise.slope * forest + flow * forest + wetland * tmax.stream + (1|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa"))
summary(glmm.M01)

# plot residuals of glmm.M1 by HUC10 basin
plot(df.fit$fHuc10, resid(glmm.M01), xlab="HUC 10 basin", ylab="Residuals")

# varying intercept & flow
glmm.M02 <-  glmer(pres ~ flow  + tmax.stream + rise.slope + forest + wetland + surfC + dams + hydroAB     + tmax.stream * forest + rise.slope * forest + flow * forest + wetland * tmax.stream + (1 + flow|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa"))
summary(glmm.M02)

# varying intercept & forest
glmm.M03 <-  glmer(pres ~ flow + tmax.stream + rise.slope + forest + wetland + surfC + dams + hydroAB + tmax.stream * forest + rise.slope * forest + flow * forest + wetland * tmax.stream + (1 + forest|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa"))
summary(glmm.M03)

# varying intercept & rise.slope
glmm.M04 <-  glmer(pres ~ flow  + tmax.stream + rise.slope + forest + wetland + surfC + dams + hydroAB + tmax.stream * forest + rise.slope * forest + flow * forest + wetland * tmax.stream + (1 + rise.slope|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa"))
summary(glmm.M04)

# varying intercept & tmax.stream
glmm.M05 <-  glmer(pres ~ flow  + tmax.stream + rise.slope + forest + wetland + surfC + dams + hydroAB     + tmax.stream * forest + rise.slope * forest + flow * forest + wetland * tmax.stream + (1 + tmax.stream|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa"))

# varying intercept & wetland
glmm.M06 <-  glmer(pres ~ flow  + tmax.stream + rise.slope + forest + wetland + surfC + dams + hydroAB + tmax.stream * forest + rise.slope * forest + flow * forest + wetland * tmax.stream + (1 + wetland|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa"))

# compare models using AIC
AIC(glmm.M01, glmm.M02, glmm.M03, glmm.M04, glmm.M05, glmm.M06)

# plot residuals by HUC10 basin
plot(df.fit$fHuc10, resid(glmm.M03), xlab="HUC 10 basin", ylab="Residuals")
```
## The model (M03) with varying-intercept & forest, with all covariates is better than intercept-only model


## Add more random effects to varying-intercept & forest model
```{r glmm comparison 2}
# varying intercept & forest & flow
glmm.M11 <-glmer(pres ~ flow + tmax.stream + rise.slope + forest + wetland + surfC + dams + hydroAB + tmax.stream * forest + rise.slope * forest + flow * forest + wetland * tmax.stream + (1 + forest + flow|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa"))

# varying intercept & forest & rise.slope
glmm.M12 <- glmer(pres ~ flow + tmax.stream + rise.slope + forest + wetland + surfC + dams + hydroAB + tmax.stream * forest + rise.slope * forest + flow * forest + wetland * tmax.stream + (1 + forest + rise.slope|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa"))

# varying intercept & forest & tmax.stream
glmm.M13 <- glmer(pres ~ flow + tmax.stream + rise.slope + forest + wetland + surfC + dams + hydroAB + tmax.stream * forest + rise.slope * forest + flow * forest + wetland * tmax.stream + (1 + forest + tmax.stream|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa"))

# varying intercept & forest & surfC
glmm.M14 <- glmer(pres ~ flow + tmax.stream + rise.slope + forest + wetland + surfC + dams + hydroAB + tmax.stream * forest + rise.slope * forest + flow * forest + wetland * tmax.stream + (1 + forest + surfC|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa"))

# varying intercept & forest & wet
glmm.M15 <- glmer(pres ~ flow + tmax.stream + rise.slope + forest + wetland + surfC + dams + hydroAB + tmax.stream * forest + rise.slope * forest + flow * forest + wetland * tmax.stream + (1 + forest + wetland|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa"))


# compare models using AIC
AIC(glmm.M03, glmm.M11, glmm.M12, glmm.M13, glmm.M14, glmm.M15)

# Both random wetland (M15) and surfC (M14) are better models than just random slope so try adding both to forest in random effect

# varying intercept & forest & wetland & surfC
glmm.M21 <- glmer(pres ~ flow + tmax.stream + rise.slope + forest + wetland + surfC + dams + hydroAB + tmax.stream * forest + rise.slope * forest + flow * forest + wetland * tmax.stream + (1 + forest + wetland + surfC|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa"))

AIC(glmm.M03, glmm.M14, glmm.M15, glmm.M21) # M23 is the best with forest and wetland and surfC

```


## Now that random effects are set, compare reduced models
```{r glmm comparison 3: substracting interactions}
# remove interactions

# remove tmax.stream * forest 
glmm.M31 <- glmer(pres ~ flow + tmax.stream + rise.slope + forest + wetland + surfC + dams + hydroAB + rise.slope * forest + flow * forest + wetland * tmax.stream + (1 + forest + wetland + surfC|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa"))

# remove rise.slope * forest 
glmm.M32 <- glmer(pres ~ flow + tmax.stream + rise.slope + forest + wetland + surfC + dams + hydroAB + tmax.stream * forest + flow * forest + wetland * tmax.stream + (1 + forest + wetland + surfC|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa"))

# remove flow * forest
glmm.M33 <- glmer(pres ~ flow + tmax.stream + rise.slope + forest + wetland + surfC + dams + hydroAB + tmax.stream * forest + rise.slope * forest + wetland * tmax.stream + (1 + forest + wetland + surfC|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa"))

# remove wetland * tmax.stream
glmm.M34 <- glmer(pres ~ flow + tmax.stream + rise.slope + forest + wetland + surfC + dams + hydroAB + tmax.stream * forest + rise.slope * forest + flow * forest + (1 + forest + wetland + surfC|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa"))

AIC(glmm.M21, glmm.M31, glmm.M32, glmm.M33, glmm.M34) 

# removing each interaction was better or no different so remove all

# No interactions
glmm.M35 <- glmer(pres ~ flow + tmax.stream + rise.slope + forest + wetland + surfC + dams + hydroAB + (1 + forest + wetland + surfC|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa"))

AIC(glmm.M31, glmm.M32, glmm.M33, glmm.M34, glmm.M35) # No interactions best

```

```{r model comparison 4 - remove individual covariates that were moderately correlated or we were skeptical about a priori}
# No rising slope
glmm.M41 <- glmer(pres ~ flow + tmax.stream + forest + wetland + surfC + dams + hydroAB + (1 + forest + wetland + surfC|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa")) 

# No dams
glmm.M42 <- glmer(pres ~ flow + tmax.stream + rise.slope + forest + wetland + surfC + hydroAB + (1 + forest + wetland + surfC|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa"))

# No hydrologic/soil/bedrock characteristics
glmm.M43 <- glmer(pres ~ flow + tmax.stream + rise.slope + forest + wetland + dams + (1 + forest + wetland + surfC|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa"))

# Simple model
glmm.M44 <- glmer(pres ~ flow + tmax.stream + rise.slope + forest + wetland + (1 + forest + wetland|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa"))

# compare models using AIC
AIC(glmm.M35, glmm.M41, glmm.M42, glmm.M43, glmm.M44) # M35 best

# plot residuals by HUC10 basin
plot(df.fit$fHuc10, resid(glmm.M35), xlab="HUC 10 basin",ylab="Residuals")

summary(glmm.M35) # model M35 is the best. Use as final model
```
Use model 35

# compare with tmin and flow
```{r comparison 5}
# compare to using tmin
#glmm.M51 <- glmer(pres ~ flow + tmax.stream + rise.slope + fall.slope + forest + wetland + surfC + dams + hydroAB + (1 + forest + wetland + surfC + fall.slope|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa"))

# compare to precip and drainage area and air temperature (climate model)
glmm.M51 <- glmer(pres ~ area + precip + tmax + forest + wetland + surfC + dams + hydroAB + (1 + forest + wetland + surfC|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa"))

# compare to flow model with just tmax and not rising slope (more directly compareable to climate model)
glmm.M52 <- glmer(pres ~ flow + tmax.stream + forest + wetland + surfC + dams + hydroAB + (1 + forest + wetland + surfC|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa")) # slighly better than climate model

# compare to falling slope
glmm.M53 <- glmer(pres ~ flow + tmax.stream + fall.slope + forest + wetland + surfC + dams + hydroAB + (1 + forest + wetland + surfC|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa"))

AIC(glmm.M35, glmm.M51, glmm.M52, glmm.M53) # stream model much better in terms of AIC. A comparison of how well they predict the validation data will be more important

save(df.fit, glmm.M35, file='boot_data.RData')
```

## Check to see HUC-level temperature is correlated with intercept values
```{r intercept vs air/precip}
huc.fit <- data.frame(row.names(coef(glmm.M35)$fHuc10), coef(glmm.M35)$fHuc10[,1])
names(huc.fit) <- c("huc10", "intercept")

huc.int.temp <- merge(huc.fit, df.huc, by = "huc10")

plot(huc.int.temp$tmax.huc, huc.int.temp$intercept,
     main="HUC10-level air temperature", xlab="tmax_ann", ylab="Intercept values in glmer")
lines(smooth.spline(huc.int.temp$tmax.huc, huc.int.temp$intercept))

```
HUC-level temperature not correlated to random intercepts


## Validation
```{r validation}

########## Temp model for Ana #######
glmm.M35 <- glmer(pres ~ flow + tmax.stream + rise.slope + forest + wetland + dams + (1 + forest + wetland|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa"))
summary(glmm.M35)

glmm.M51 <- glmer(pres ~ area + precip + tmax + forest + wetland + surfC + dams + hydroAB + (1 + forest + wetland|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa"))
summary(glmm.M51)

Pairs <- df.fit[ , c("pres", "area", "forest", "wetland", "rise.slope", "fall.slope", "tmax.stream", "tmax", "precip", "surfC", "hydroAB", "flow", "dams")]
pairs(Pairs, upper.panel=panel.smooth, lower.panel=panel.cor, diag.panel=panel.hist)

AIC(glmm.M35, glmm.M51)

###################################

pred.valid <- inv.logit(predict(glmm.M35, df.valid, allow.new.levels = TRUE))

auc(sensitivity(pred.valid, as.factor(df.valid$pres))) # 0.613
auc(specificity(pred.valid, as.factor(df.valid$pres))) # 0.639
auc(accuracy(pred.valid, as.factor(df.valid$pres)))    # 0.624
auc(roc(pred.valid, as.factor(df.valid$pres)))         # 0.752
plot(sensitivity(pred.valid, as.factor(df.valid$pres)))
plot(specificity(pred.valid, as.factor(df.valid$pres)))
plot(accuracy(pred.valid, as.factor(df.valid$pres)))
plot(roc(pred.valid, as.factor(df.valid$pres)))
par(mfrow = c(1,1))

# AUC for climate model validation
pred.valid2 <- inv.logit(predict(glmm.M51, df.valid, allow.new.levels = TRUE))
auc(sensitivity(pred.valid2, as.factor(df.valid$pres))) # 0.616
auc(specificity(pred.valid2, as.factor(df.valid$pres))) # 0.643
auc(accuracy(pred.valid2, as.factor(df.valid$pres)))    # 0.628
auc(roc(pred.valid2, as.factor(df.valid$pres)))         # 0.759

streamV.roc <- roc(pred.valid, as.factor(df.valid$pres))
climateV.roc <- roc(pred.valid2, as.factor(df.valid$pres))

par(mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
plot(streamV.roc)
plot(climateV.roc, add=T, col='red')
legend(0.6, 0.2, legend=c("Environmental", "Climate"), col=c(1,2), lty=c(1,1))
```


## ROC AUC
```{r ROC AUC}

#####################################
aucFun <- function(.){
  library(boot)
  library(ROCR)
  pred <- inv.logit(predict(.))
  pROCR <- prediction(pred, .@resp$y)
  AUC <- unlist(performance(pROCR, "auc")@y.values)
  ROC <- unlist(performance(pROCR, "tpr", "fpr")@y.values)
  return(AUC)
}

rocFun <- function(.){
  library(boot)
  library(ROCR)
  pred <- inv.logit(predict(.))
  pROCR <- prediction(pred, .@resp$y)
  ROC <- unlist(performance(pROCR, "tpr", "fpr")@y.values)
  return(ROC)
}

coefFun <- function(fit){
  return(c(fixef(fit),unlist(VarCorr(fit))))
}

test <- aucFun(glmm.M35)

system.time(auc.boot <- bootMer(glmm.M35, aucFun, nsim=4, use.u=F, seed=3453, parallel="multicore", ncpus=4))
(bCI <- boot.ci(auc.boot, index =c(1,1), type="norm"))
hist(auc.boot$t)
abline(v = auc.boot$t0, col='red', lwd=3)

system.time(roc.boot <- bootMer(glmm.M35, rocFun, nsim=4, use.u=F, seed=3453, parallel="multicore", ncpus=4))
(bCI <- boot.ci(roc.boot, index =c(1,1), type="norm"))
hist(roc.boot$t)
abline(v = roc.boot$t0, col='red', lwd=3)

system.time(coef.boot <- bootMer(glmm.M35, coefFun, nsim=4, use.u=F, seed=3453, parallel="multicore", ncpus=4))
coef.boot
(bCI <- boot.ci(coef.boot, index =c(1,1), type="norm"))
hist(auc.boot$t)
abline(v = auc.boot$t0, col='red', lwd=3)

glmm.M35v <- glmer(formula(glmm.M35), family = binomial(link = "logit"), data = df.valid, control = glmerControl(optimizer="bobyqa"))
system.time(auc.boot.M35v <- bootMer(glmm.M35v, aucFun, nsim=4, use.u=F, seed=3453, parallel="multicore", ncpus=4))
(bCI.M35v <- boot.ci(auc.boot.M35v, index =c(1,1), type="norm"))
hist(auc.boot.M35v$t)
abline(v = auc.boot.M35v$t0, col='red', lwd=3)

glmm.M51v <- update(glmm.M51, .~., data=df.valid)

system.time(roc.boot.M35 <- bootMer(glmm.M35, rocFun, nsim=1000, use.u=F, seed=3453, parallel="multicore", ncpus=4, ))
system.time(roc.boot.M35v <- bootMer(glmm.M35v, rocFun, nsim=1000, use.u=F, seed=3453, parallel="multicore", ncpus=4))
system.time(roc.boot.M51 <- bootMer(glmm.M51, rocFun, nsim=1000, use.u=F, seed=3453, parallel="multicore", ncpus=4))
system.time(roc.boot.M51v <- bootMer(glmm.M51v, rocFun, nsim=1000, use.u=F, seed=3453, parallel="multicore", ncpus=4))

system.time(auc.boot.M35 <- bootMer(glmm.M35, aucFun, nsim=1000, use.u=F, seed=3453, parallel="multicore", ncpus=4, ))
system.time(auc.boot.M35v <- bootMer(glmm.M35v, aucFun, nsim=1000, use.u=F, seed=3453, parallel="multicore", ncpus=4))
system.time(auc.boot.M51 <- bootMer(glmm.M51, aucFun, nsim=1000, use.u=F, seed=3453, parallel="multicore", ncpus=4))
system.time(auc.boot.M51v <- bootMer(glmm.M51v, aucFun, nsim=1000, use.u=F, seed=3453, parallel="multicore", ncpus=4))


aucSummary <- function(auc.boot, Mean=T, CI=c(0.025, 0.975), SE=F){
  library(AUC)
  auc.out <- auc.boot$t
    auc.CI <- quantile(auc.out, probs=CI, na.rm=T)
    auc.mean <- NA
    auc.se <- NA
    if(Mean == T){
      auc.mean <- mean(auc.out, na.rm=T)
    }
    if(SE == T) {
      auc.se <- sd(auc.out, na.rm=T)/sqrt(length(auc.out))
    }
    auc.summary <- data.frame(auc.boot$t0, auc.mean, auc.CI[1], auc.CI[2], auc.se)
    names(auc.summary) <- c('AUC', 'Mean', 'LCI', 'UCI', 'SE')
    row.names(auc.summary) <- deparse(substitute(auc.boot))
    return(auc.summary)
  } # end function

auc35 <- aucSummary(auc.boot.M35, SE=T)
auc35v <- aucSummary(auc.boot.M35v, SE=T)
auc51 <- aucSummary(auc.boot.M51, SE=T)
auc51v <- aucSummary(auc.boot.M51v, SE=T)

auc.comp <- data.frame(rbind(auc35, auc51, auc35v, auc51v))
auc.comp$Model <- as.factor(row.names(auc.comp))
row.names(auc.comp) <- c("Environmental", "Climate", "Environmental Validation", "Climate Validation")

se <- ggplot(auc.comp, aes(Model, Mean,
  ymin = LCI, ymax=UCI, colour = Model))
se + geom_linerange() + geom_pointrange() + geom_point(aes(Model, AUC), colour = 'black')

########################################

par(mfrow = c(1,1))
plot(perf35v, lty=3, col="blue")
abline(0, 1)
plot(perf51v, lty=3, col="green", add=TRUE)
plot(perf35, lty=3, col="black", add=TRUE)
plot(perf51, lty=3, col="red", add=TRUE)
plot(perf35, avg="vertical", lwd=2, col="black", spread.estimate="stderror", plotCI.lwd=2, add=TRUE)
plot(perf35v, avg="vertical", lwd=2, col="blue", spread.estimate="stderror", plotCI.lwd=2, add=TRUE)
plot(perf51, avg="vertical", lwd=2, col="red", spread.estimate="stderror", plotCI.lwd=2, add=TRUE)
plot(perf51v, avg="vertical", lwd=2, col="green", spread.estimate="stderror", plotCI.lwd=2, add=TRUE)
legend(0.55, 0.4, c('Flow (Training)','Flow (Validation)', 'Climate (Training)', 'Climate (Validation)'), col=c('black','blue', 'red', 'green'), lwd=2)

rbind(auc35, auc35v, auc51, auc51v) # output table

quantile(unlist(performance(p35, "auc")@y.values), c(0.025, 0.5, 0.975))
quantile(unlist(performance(p35, "rmse")@y.values), c(0.025, 0.5, 0.975))


pred.fit <- inv.logit(predict(glmm.M35, df.fit, allow.new.levels = TRUE, interval="prediction"))
auc(sensitivity(pred.fit, as.factor(df.fit$pres))) # 0.681
auc(specificity(pred.fit, as.factor(df.fit$pres))) # 0.694
auc(accuracy(pred.fit, as.factor(df.fit$pres)))    # 0.687
auc(roc(pred.fit, as.factor(df.fit$pres)))         # 0.875
auc(roc(as.numeric(pred.list$predictions[[1]]), as.factor(pred.list$observed[[1]])))
mean(unlist(performance(p35, "auc")@y.values))

par(mfrow=c(1, 2))
hist(unlist(pred.M35$predictions), freq=F, breaks=20)
hist(pred.fit, freq=F, breaks=20)
par(mfrow=c(1,1))

par(mfrow = c(2,2))
plot(sensitivity(pred.fit, as.factor(df.fit$pres)))
plot(specificity(pred.fit, as.factor(df.fit$pres)))
plot(accuracy(pred.fit, as.factor(df.fit$pres)))
plot(roc(pred.fit, as.factor(df.fit$pres)))
par(mfrow=c(1,1))

# AUC for climate model
pred.fit2 <- inv.logit(predict(glmm.M51, df.fit, allow.new.levels = TRUE))
auc(sensitivity(pred.fit2, as.factor(df.fit$pres))) # 0.679
auc(specificity(pred.fit2, as.factor(df.fit$pres))) # 0.691
auc(accuracy(pred.fit2, as.factor(df.fit$pres)))    # 0.685
auc(roc(pred.fit2, as.factor(df.fit$pres)))         # 0.870
```

## Prediction for all catchments
```{r process and standardize all catchment data}
all.data2 <- data[ , c("PresenceAbsence", "TotDASqKM", "ReachElevationM", "ReachSlopeDEG", "Forest", "Agriculture", "CONUSWetland", "CONUSOpenWater", "Developed", "AnnualTmaxC", "AnnualTminC", "WinterPrcpMM", "SummerPrcpMM", "AnnualPrcpMM", "SurficialCoarseC", "HydrologicGroupAB", "StreamOrder", "pred_flow", "SummerMaxT", "RiseSlope", "FallSlope", "TNC_DamCount","Latitude", "Longitude", "HUC_8", "HUC_12", "FEATUREID")] 

names(all.data2) <- c("pres", "area", "elev", "slope", "forest", "ag", "wetland", "water", "develop", "tmax", "tmin", "precip.winter", "precip.summer", "precip", "surfC", "hydroAB", "stream.order", "flow", "tmax.stream", "rise.slope", "fall.slope", "dams", "lat", "lon", "HUC_8", "HUC_12", "FEATUREID")

all.data2$HUC_10 <- substr(all.data2$HUC_12, 1, 10)
all.data2 <- subset(all.data2, !is.na(HUC_10))

all.data2$state <- latlong2state(all.data2[ , c("lon", "lat")])

all.data3 <- all.data2[all.data2$area <= 50, ]
all.data3 <- all.data3[which(all.data3$rise.slope > 0), ]
all.data3 <- all.data3[which(all.data3$fall.slope > 0), ]

all.data.fit <- all.data3[ , c("FEATUREID","area","precip", "precip.summer", "tmax.stream", "rise.slope", "fall.slope", "tmax", "tmin", "surfC", "forest", "ag", "slope","pres", "hydroAB", "wetland", "water", "elev", "develop", "dams", "flow", "state", "HUC_8","HUC_10")] # ,"state","stateID"

all.data.fit2 <- subset(all.data.fit, is.na(slope) == FALSE)
all.data.fit2 <- subset(all.data.fit2, is.na(elev) == FALSE)
all.data.fit2 <- subset(all.data.fit2, is.na(tmax.stream) == FALSE)

summary(all.data.fit2) # no NA remaining

AllHuc10 <- subset(all.data.fit2, !duplicated(HUC_10, fromLast=FALSE))
AllHuc10$HUC_10n <- as.numeric(AllHuc10$HUC_10) # just to order below. to be deleted
AllHuc10 <- AllHuc10[ order(AllHuc10$HUC_10n), ]
AllHuc10 <- data.frame(cbind(AllHuc10$HUC_10, c(1:nrow(AllHuc10))))
names(AllHuc10) <- c("HUC_10","fHuc10")
str(AllHuc10)
AllHuc10$HUC_10 <- as.character(AllHuc10$HUC_10)

## add HUC10 ID to fit df
all.data.fit2 <- merge(x=all.data.fit2, y=AllHuc10, by="HUC_10")

all.data.fit2$huc10 <- as.numeric(all.data.fit2$fHuc10)
bar <- NA
for(i in 1:max(all.data.fit2$huc10)) {
  foo <- subset(all.data.fit2, subset = huc10 == i)
  bar[i] <- sum(foo$area * foo$tmax) / sum(foo$area)
}

df.huc <- data.frame(1:max(all.data.fit2$huc10), bar)
names(df.huc) <- c("huc10", "tmax.huc")

all.data.fit2 <- merge(all.data.fit2, df.huc, by = "huc10")

all.data.fit2 <- all.data.fit2[c("pres", "tmax.huc", "area", "precip", "tmax.stream", "rise.slope", "fall.slope", "tmax", "tmin", "forest", "slope", "surfC", "ag", "precip.summer", "elev", "hydroAB", "develop", "flow", "wetland", "water", "FEATUREID", "state", "fHuc10", "HUC_8", "HUC_10", "dams")]

# Find means and sd used in standardizing the data used for model fitting
Means <- apply(data.fit2[ , 2:20], MARGIN=2, FUN = mean, na.rm=T)
SDs <- apply(data.fit2[ , 2:20], MARGIN=2, FUN = sd, na.rm=T)

# Standardize the continuous covariates for all catchments
all.temp <- data.frame(matrix(NA, dim(all.data.fit2)[1], length(Means)))
for(i in 1:length(Means)){
  all.temp[ , i] <- (all.data.fit2[ ,i+1] - Means[i]) / SDs[i]
}
names(all.temp) <- names(Means)

# recombine with other covariates in the same order as the fitted data
all.data.fit.std <- cbind(all.data.fit2[ , c(1, 21:26)], all.temp)

str(all.data.fit.std)
summary(all.data.fit.std) # appears to be some outliers. Find and remove.

hist(all.data.fit.std$surfC, breaks=50) # not super clear outliers but could exclude those over 4 or 5
hist(all.data.fit.std$wetland); boxplot(all.data.fit.std$wetland) # could cut off at 6

all.data.fit.std <- all.data.fit.std[all.data.fit.std$wetland <= 6, ]
df.fit.all <- all.data.fit.std[all.data.fit.std$surfC <= 6, ]


```


```{r}

# made standardized dataframe of all catchments we want to project to

pred.fit.climate.all <- inv.logit(predict(glmm.M51, df.fit.all, allow.new.levels = TRUE))
pred.fit.stream.all <- inv.logit(predict(glmm.M35, df.fit.all, allow.new.levels = TRUE))

deltas <- pred.fit.climate.all - pred.fit.stream.all

summary(deltas)
sd(deltas)
quantile(deltas, probs = c(0.025, 0.5, 0.975))

fits <- data.frame(FEATUREID = df.fit.all$FEATUREID, pres = df.fit.all$pres, deltas = deltas, psi.clim = pred.fit.climate.all, psi.stream = pred.fit.stream.all)

fits <- merge(fits, LocalStats[ , c("FEATUREID", "Latitude", "Longitude")], by = "FEATUREID")
names(fits) <- c("FEATUREID", "pres", "deltas", "psi.clim", "psi.stream", "lat", "lon")

fits <- merge(fits, df.fit.all, by = "FEATUREID") 
fits <- merge(fits, data.fit2.all, by = "FEATUREID") 

save(fits, file = "Predictions.RData")

summary(fits)
dim(fits)

ggplot(data = fits, aes(psi.clim, psi.stream)) + geom_point()

par(mar = c(3.5,3,2,1), mgp = c(2,.7,0), tck = -.02)
plot(fits$psi.clim, fits$psi.stream, xlab = "Climate Predicted Occupancy",
     ylab = "Stream Temperature/Flow Predicted Occupancy")
abline(0, 1, col = "red")

par(mfrow = c(1, 3))
plot(fits$forest, fits$psi.clim)
lines(smooth.spline(fits$forest, fits$psi.clim), col = "red")
plot(fits$asin.forest, fits$psi.clim)
lines(seq(-3, 2, by = 0.1), inv.logit(fixef(glmm.simple)[1] + fixef(glmm.simple)["asin.forest"]*(seq(-3, 2, by = 0.1))), col = "red")
plot(fits$asin.forest, fits$psi.stream)
lines(seq(-3, 2, by = 0.1), inv.logit(fixef(glmm.streamT.flow2)[1] + fixef(glmm.streamT.flow2)["asin.forest"]*(seq(-3, 2, by = 0.1))), col = "red")
par(mfrow = c(1,1))

fixed <- data.frame(c(m1[1:3], NA, m1[4], NA, m1[5]), c(m2[1], NA, NA, m2[2], m2[3:5]))
names(fixed) <- c("Climate", "Stream")
row.names(fixed) <- c("Intercept", "log.area", "precip", "log.flow", "tmax", "rise.slope", "asin.forest")
fixed


library(ggmap)
map.center <- geocode("Worcester, MA")
baseMap <- qmap(c(lon=map.center$lon, lat=map.center$lat), source="google", zoom=7)
baseMap + geom_point(aes(x=lon, y=lat, 
                                size=(abs(deltas)),
                                color=factor(pres)), 
                            data=fits[which(abs(fits$deltas) <= 0.3), ]) +
  scale_color_manual(values=c('darkred','darkgreen')) 
  #ggtitle( as.character(y)))

```

## Semivariograms (Torgeogram)
```{r Torgegram}
df.fit.all$psi.climate <- inv.logit(predict(glmm.M51, df.fit.all, allow.new.levels = TRUE))
df.fit.all$psi.stream <- inv.logit(predict(glmm.M35, df.fit.all, allow.new.levels = TRUE))

data.geo <- merge(df.fit.all[ , c("FEATUREID", "psi.stream", "psi.climate", "pres")], data[ , c("FEATUREID", "Latitude", "Longitude")], by="FEATUREID", all.x=T)
str(data.geo)
summary(data.geo)

# dists <- dist(data.geo[ , c("Latitude", "Longitude")])
# summary(dists) 
#     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
# 0.000266 0.822900 1.300000 1.356000 1.834000 3.975000 

rm(dists) # vector length = 331,286,670 (kills laptop)

library(geoR)
df.geo <- as.geodata(data.geo, coords.col=c(5,6), data.col=c(2,3))
str(df.geo)

#plot(df.geo)
breaks = seq(0, 2, length.out = 200)
Torge <- variog(df.geo, breaks=breaks, pairs.min=10)

v1.summary <- cbind(c(1:10), Torge$v, Torge$n)
colnames(v1.summary) <- c("lag", "semi-variance", "semi-variance", "# of pairs")
v1.summary
plot(Torge, type='b', pch = c(1,2))
rm(Torge)

hawkins.cressie <- variog(df.geo, estimator.type="modulus")
plot(hawkins.cressie)
rm(hawkins.cressie)

# just look at small distances (within ~10km)
breaks = seq(0, 0.1, length.out = 50)
Torge.sm <- variog(df.geo, breaks=breaks)
v1.summary <- cbind(c(1:(length(breaks)-1)), Torge.sm$v, Torge.sm$n)
colnames(v1.summary) <- c("lag", "semi-variance", "semi-variance", "# of pairs")
v1.summary
plot(Torge.sm, type='l')
rm(Torge.sm)

# Semivariogram of P/A data

df.geo.pres <- as.geodata(data.geo, coords.col=c(5,6), data.col=c(2,3,4))
breaks = seq(0, 1, length.out = 25)
Torge <- variog(df.geo.pres, breaks=breaks, pairs.min=10)

v1.summary <- cbind(c(1:10), Torge$v, Torge$n)
colnames(v1.summary) <- c("lag", "semi-variance", "semi-variance", "# of pairs")
v1.summary
plot(Torge, type='b', pch = c(1,2,3))
rm(Torge)

# Semivariograms for observed P/A compared with predicted occupancy

# Semivariograms for predictor variables


```


## Prediction for all catchments in Future Climates
```{r}
# changed UpstreamStats to UpstreamStatsPred in the below file before importing
load("nenyUMassProjectionsTmax.Rdata")

df.fit.all <- merge(data.fit.std, LocalStatsPred, by = "FEATUREID", all.x = TRUE)

df.fit.all$tmax <- (df.fit.all$tmax2010rcp45 - mean(df.fit.all$tmax2010rcp45, na.rm = TRUE)) / sd(df.fit.all$tmax2010rcp45, na.rm = TRUE)

# Simple model for Meeting
glmm.simple <- glmer(pres ~ log.area  + precip  + tmax + asin.forest + (1 + asin.forest|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa"))
summary(glmm.simple)

glmm.streamT.flow2 <- glmer(pres ~ log.flow + tmax.stream + rise.slope + asin.forest + (1 + asin.forest|fHuc10), family = binomial(link = "logit"), data = df.fit, control = glmerControl(optimizer="bobyqa"))
summary(glmm.streamT.flow2)


tmax.mean <- mean(df.fit.all$tmax2010rcp45, na.rm = TRUE)
tmax.sd <- sd(df.fit.all$tmax2010rcp45, na.rm = TRUE)

df.fit.all$tmax <- (df.fit.all$tmax2010rcp45 - tmax.mean) / tmax.sd
df.fit.all$pred.2010rcp45 <- inv.logit(predict(glmm.simple, df.fit.all, allow.new.levels = TRUE))

df.fit.all$tmax <- (df.fit.all$tmax2030rcp45 - tmax.mean) / tmax.sd
df.fit.all$pred.2030rcp45 <- inv.logit(predict(glmm.simple, df.fit.all, allow.new.levels = TRUE))

df.fit.all$tmax <- (df.fit.all$tmax2080rcp45 - tmax.mean) / tmax.sd
df.fit.all$pred.2080rcp45 <- inv.logit(predict(glmm.simple, df.fit.all, allow.new.levels = TRUE))

df.fit.all$tmax <- (df.fit.all$tmax2030rcp85 - tmax.mean) / tmax.sd
df.fit.all$pred.2030rcp85 <- inv.logit(predict(glmm.simple, df.fit.all, allow.new.levels = TRUE))

df.fit.all$tmax <- (df.fit.all$tmax2080rcp85 - tmax.mean) / tmax.sd
df.fit.all$pred.2080rcp85 <- inv.logit(predict(glmm.simple, df.fit.all, allow.new.levels = TRUE))


fits <- merge(UpstreamStats[ , c("FEATUREID", "Latitude", "Longitude")], df.fit.all[ , c("FEATUREID", "pred.2010rcp45", "pred.2030rcp45", "pred.2080rcp45", "pred.2030rcp85", "pred.2080rcp85")], by = "FEATUREID")

save(fits, file = "Predictions_2014-04-23.RData")


auc(roc(pred.fit.climate, as.factor(df.fit$pres))) # 0.842
auc(roc(pred.fit.stream, as.factor(df.fit$pres)))  # 0.850

deltas <- pred.fit.climate.all - pred.fit.stream.all

summary(deltas)
sd(deltas)
quantile(deltas, probs = c(0.025, 0.5, 0.975))

fits <- data.frame(FEATUREID = df.fit.all$FEATUREID, pres = df.fit.all$pres, deltas = deltas, psi.clim = pred.fit.climate.all, psi.stream = pred.fit.stream.all)

fits <- merge(fits, LocalStats[ , c("FEATUREID", "Latitude", "Longitude")], by = "FEATUREID")
names(fits) <- c("FEATUREID", "pres", "deltas", "psi.clim", "psi.stream", "lat", "lon")

fits <- merge(fits, df.fit.all, by = "FEATUREID") 
fits <- merge(fits, data.fit2.all, by = "FEATUREID") 

save(fits, file = "Predictions.RData")

summary(fits)
dim(fits)

ggplot(data = fits, aes(psi.clim, psi.stream)) + geom_point()

par(mar = c(3.5,3,2,1), mgp = c(2,.7,0), tck = -.02)
plot(fits$psi.clim, fits$psi.stream, xlab = "Climate Predicted Occupancy",
     ylab = "Stream Temperature/Flow Predicted Occupancy")
abline(0, 1, col = "red")

par(mfrow = c(1, 3))
plot(fits$forest, fits$psi.clim)
lines(smooth.spline(fits$forest, fits$psi.clim), col = "red")
plot(fits$asin.forest, fits$psi.clim)
lines(seq(-3, 2, by = 0.1), inv.logit(fixef(glmm.simple)[1] + fixef(glmm.simple)["asin.forest"]*(seq(-3, 2, by = 0.1))), col = "red")
plot(fits$asin.forest, fits$psi.stream)
lines(seq(-3, 2, by = 0.1), inv.logit(fixef(glmm.streamT.flow2)[1] + fixef(glmm.streamT.flow2)["asin.forest"]*(seq(-3, 2, by = 0.1))), col = "red")
par(mfrow = c(1,1))

fixed <- data.frame(c(m1[1:3], NA, m1[4], NA, m1[5]), c(m2[1], NA, NA, m2[2], m2[3:5]))
names(fixed) <- c("Climate", "Stream")
row.names(fixed) <- c("Intercept", "log.area", "precip", "log.flow", "tmax", "rise.slope", "asin.forest")
fixed

```


### Effect of forest
```{r effect of forest on occ}
# simulate coef values
n.sims=1000
simCoef <- as.data.frame(fixef(sim(glmm.M35, n.sims=n.sims)))
names(simCoef) <- names(fixef(glmm.M35))

# Plot effect of catchment forest on occurrence prob at a typical HUC10 basin # Gelman p. 44
eff.forest <- data.frame(forest.raw=seq(0,100,length.out=100), tmax.stream=rep(0,100), flow=rep(0,100), rise.slope=rep(0,100))
eff.forest$forest <- (eff.forest$forest.raw - mean(data.fit2$forest, na.rm=T))/sd(data.fit2$forest, na.rm=T)

sim.prob.forest <- matrix(NA, nrow=nrow(eff.forest), ncol=n.sims)
for (i in 1:n.sims){
  sim.prob.forest[,i] <- exp(simCoef[i,1] + simCoef[i,"forest"]*eff.forest$forest) / (1 + exp(simCoef[i,1] + simCoef[i,"forest"]*eff.forest$forest))
}
sim.prob.forest <- as.data.frame(sim.prob.forest)

eff.forest$mean <- apply(sim.prob.forest[,1:n.sims], 1, mean)
eff.forest$lower <- apply(sim.prob.forest[,1:n.sims], 1, quantile, probs=c(0.025))
eff.forest$upper <- apply(sim.prob.forest[,1:n.sims], 1, quantile, probs=c(0.975))

ggplot(eff.forest, aes(x = forest.raw, y = mean)) + 
  geom_ribbon(aes(ymin = lower, ymax = upper), fill="grey") +
  geom_line(colour = "black", size = 2) +
  #labs(title = "Occupancy in CT, MA, NH & NY") +
  xlab("Percent forest cover upstream") +
  ylab("Occupancy probability") +
  theme_bw() + 
  ylim(0, 1) +
  theme(axis.text.y = element_text(size=15),
        axis.text.x = element_text(size=15),
        axis.title.x = element_text(size=17, face="bold"),
        axis.title.y = element_text(size=17, angle=90, face="bold"),
        plot.title = element_text(size=20))
```


### Effect of rise.slope
```{r effect of rise.slope on occ}
# simulate coef values
n.sims=1000
simCoef <- as.data.frame(fixef(sim(glmm.M35, n.sims=n.sims)))
names(simCoef) <- names(fixef(glmm.M35))

# Plot effect of catchment rise.slope on occurrence prob at a typical HUC10 basin # Gelman p. 44
eff.rise.slope <- data.frame(rise.slope.raw=seq(0.5,1,length.out=100), forest=rep(0,100), flow=rep(0,100), rise.slope=rep(0,100))
eff.rise.slope$rise.slope <- (eff.rise.slope$rise.slope.raw - mean(data.fit2$rise.slope, na.rm=T))/sd(data.fit2$rise.slope, na.rm=T)

sim.prob.rise.slope <- matrix(NA, nrow=nrow(eff.rise.slope), ncol=n.sims)
for (i in 1:n.sims){
  sim.prob.rise.slope[,i] <- exp(simCoef[i,1] + simCoef[i,"rise.slope"]*eff.rise.slope$rise.slope) / (1 + exp(simCoef[i,1] + simCoef[i,"rise.slope"]*eff.rise.slope$rise.slope))
}
sim.prob.rise.slope <- as.data.frame(sim.prob.rise.slope)

eff.rise.slope$mean <- apply(sim.prob.rise.slope[,1:n.sims], 1, mean)
eff.rise.slope$lower <- apply(sim.prob.rise.slope[,1:n.sims], 1, quantile, probs=c(0.025))
eff.rise.slope$upper <- apply(sim.prob.rise.slope[,1:n.sims], 1, quantile, probs=c(0.975))

ggplot(eff.rise.slope, aes(x = rise.slope.raw, y = mean)) + 
  geom_ribbon(aes(ymin = lower, ymax = upper), fill="grey") +
  geom_line(colour = "black", size = 2) +
  #labs(title = "Occupancy in CT, MA, NH & NY") +
  xlab("Stream temperature sensitivity") +
  ylab("Occupancy probability") +
  scale_x_reverse() +
  theme_bw() + 
  ylim(0, 1) +
  theme(axis.text.y = element_text(size=15),
        axis.text.x = element_text(size=15),
        axis.title.x = element_text(size=17, face="bold"),
        axis.title.y = element_text(size=17, angle=90, face="bold"),
        plot.title = element_text(size=20))
```



### Effect of tmax.stream
```{r effect of tmax.stream on occ}
# simulate coef values
n.sims=1000
simCoef <- as.data.frame(fixef(sim(glmm.M35, n.sims=n.sims)))
names(simCoef) <- names(fixef(glmm.M35))

# Plot effect of catchment tmax.stream on occurrence prob at a typical HUC10 basin # Gelman p. 44
eff.tmax.stream <- data.frame(tmax.stream.raw=seq(10,30,length.out=100), forest=rep(0,100), flow=rep(0,100), rise.slope=rep(0,100))
eff.tmax.stream$tmax.stream <- (eff.tmax.stream$tmax.stream.raw - mean(data.fit2$tmax.stream, na.rm=T))/sd(data.fit2$tmax.stream, na.rm=T)

sim.prob.tmax.stream <- matrix(NA, nrow=nrow(eff.tmax.stream), ncol=n.sims)
for (i in 1:n.sims){
  sim.prob.tmax.stream[,i] <- exp(simCoef[i,1] + simCoef[i,"tmax.stream"]*eff.tmax.stream$tmax.stream) / (1 + exp(simCoef[i,1] + simCoef[i,"tmax.stream"]*eff.tmax.stream$tmax.stream))
}
sim.prob.tmax.stream <- as.data.frame(sim.prob.tmax.stream)

eff.tmax.stream$mean <- apply(sim.prob.tmax.stream[,1:n.sims], 1, mean)
eff.tmax.stream$lower <- apply(sim.prob.tmax.stream[,1:n.sims], 1, quantile, probs=c(0.025))
eff.tmax.stream$upper <- apply(sim.prob.tmax.stream[,1:n.sims], 1, quantile, probs=c(0.975))

ggplot(eff.tmax.stream, aes(x = tmax.stream.raw, y = mean)) + 
  geom_ribbon(aes(ymin = lower, ymax = upper), fill="grey") +
  geom_line(colour = "black", size = 2) +
  #labs(title = "Occupancy in CT, MA, NH & NY") +
  xlab("Annual maximum stream temperature (C)") +
  ylab("Occupancy probability") +
  theme_bw() + 
  ylim(0, 1) +
  theme(axis.text.y = element_text(size=15),
        axis.text.x = element_text(size=15),
        axis.title.x = element_text(size=17, face="bold"),
        axis.title.y = element_text(size=17, angle=90, face="bold"),
        plot.title = element_text(size=20))
```

### Effect of catchment flow
```{r effect of flow on occ}

### Effect of catchment flow
eff.flow <- data.frame(flow.raw=seq(1,50,length.out=100))
eff.flow$flow <- (eff.flow$flow.raw - mean(data.fit2$flow, na.rm=T))/sd(data.fit2$flow, na.rm=T)

sim.prob.flow <- matrix(NA, nrow=nrow(eff.flow), ncol=n.sims)
for (i in 1:n.sims){
  sim.prob.flow[,i] <- exp(simCoef[i,1] + simCoef[i,"flow"]*eff.flow$flow) / (1 + exp(simCoef[i,1] + simCoef[i,"flow"]*eff.flow$flow))
}
sim.prob.flow <- as.data.frame(sim.prob.flow)

eff.flow$mean <- apply(sim.prob.flow[,1:n.sims], 1, mean)
eff.flow$lower <- apply(sim.prob.flow[,1:n.sims], 1, quantile, probs=c(0.025))
eff.flow$upper <- apply(sim.prob.flow[,1:n.sims], 1, quantile, probs=c(0.975))

ggplot(eff.flow, aes(x = (flow.raw), y = mean)) + 
  geom_ribbon(aes(ymin = lower, ymax = upper), fill="grey") +
  geom_line(colour = "black", size = 2) +
  #labs(title = "Occupancy in CT, MA, NH & NY") +
  xlab("Mean annual flow") +
  ylab("Occupancy probability") +
  theme_bw() + 
  ylim(0, 1) +
  theme(axis.text.y = element_text(size=15),
        axis.text.x = element_text(size=15),
        axis.title.x = element_text(size=17, face="bold"),
        axis.title.y = element_text(size=17, angle=90, face="bold"),
        plot.title = element_text(size=20))
```


## Save objects and data for Ana

effect size plots
AUC boxplots
ROC curves (validation)
Variograms (data, 2 models)

```{r output}

#means <- apply(data.fit3[ ,2:18], MARGIN = 2, FUN = function(x){mean(x, na.rm = TRUE)})
#stdevs <- apply(data.fit3[ ,2:18], MARGIN = 2, FUN = function(x){sd(x, na.rm = TRUE)})

#save(data.fit2, data.fit3, means, stdevs, df.fit, glmm.simple, glmm.streamT.flow, glmm.streamT.flow2, glmm.rising, file = "Northeast_Model_Output.RData")
```

## temporal trend in occupancy probability among catchments
```{r temporal trend}
## add current & future occupancy probability to the main df
data.pred2$current <- data.pred.std$occ.prob
data.pred2$plus2 <- data.pred.std.plus2$occ.prob
data.pred2$plus4 <- data.pred.std.plus4$occ.prob
data.pred2$plus6 <- data.pred.std.plus6$occ.prob

## make a long df for ggplot2
occ.prob.long <- melt(data.pred2[c("FEATUREID","state","fHuc10","current","plus2","plus4","plus6")], 
                      id.vars=c("FEATUREID","state","fHuc10"),
                      variable.name="scenario",
                      value.name="occ.prob")

## plot temporal trend by state
ggplot(data=occ.prob.long[occ.prob.long$state %in% c('Connecticut','Massachusetts','New Hampshire','New York'), ], 
       aes(x=scenario,y=occ.prob,fill=state)) + geom_boxplot() +
  ggtitle("Distribution of mean predicted occupancy probability \nunder current and future temperature increase scenarios") +
    xlab("Scenarios") + ylab("Probability of occupancy") +
    theme(axis.text.y = element_text(size=15, colour="black"),
        axis.text.x = element_text(size=15, colour="black"),
        axis.title.x = element_text(size=17, face="bold"),
        axis.title.y = element_text(size=17, angle=90, face="bold"),
        plot.title = element_text(size=20),
        legend.title = element_text(size=16, face="bold"),
        legend.text = element_text(size=16))

## plot temporal trend by state (including ME, VT & RI)
ggplot(data=occ.prob.long[occ.prob.long$state %in% c('Connecticut','Massachusetts','New Hampshire','New York','Vermont','Maine','Rhode Island'), ], 
       aes(x=scenario,y=occ.prob,fill=state)) + geom_boxplot() +
  ggtitle("Distribution of mean predicted occupancy probability \nunder current and future temperature increase scenarios") +
    xlab("Scenarios") + ylab("Probability of occupancy") +
    theme(axis.text.y = element_text(size=15, colour="black"),
        axis.text.x = element_text(size=15, colour="black"),
        axis.title.x = element_text(size=17, face="bold"),
        axis.title.y = element_text(size=17, angle=90, face="bold"),
        plot.title = element_text(size=20),
        legend.title = element_text(size=16, face="bold"),
        legend.text = element_text(size=16))
```

## make three-dimensional array of HUC10 basin specific coef values from simulation
```{r make coef array}
### create blank array
n.sims=100; n.param=length(fixef(final.model)); n.huc=nrow(AllHuc10)  # n.param includes intercept
coefArray <- array(NA, dim = c(n.sims, n.param, n.huc), 
                   dimnames=list(1:n.sims, c("intercept","log.area","asin.forest","sqrt.slope","precip",
                                             "tmin","drainC","log.area*asin.forest",
                                             "sqrt.slope*tmin"), 1:n.huc))

### simulate coef values for each Huc10
final.model.sim <- sim(final.model, n.sims=n.sims)
coef.final.model.sim <- coef(final.model.sim)

### populate coef for fixed effects in the blank array
for (i in 1:n.huc){
  coefArray[ ,6:n.param, i] <- coef.final.model.sim$fixef[, 6:n.param]
}
coefArray[,,1:3]  # check to make sure that all are populated except first columns

### now populate random-effect columns
for (i in 1:n.huc){
  coefArray[,1:5,i] <- coef.final.model.sim$ranef$fHuc10[,i,1:5]
}
coefArray[,,1:3]  # check to make sure

### add one more array for fish data in unmodeled HUC10 basins
coefUnmod <- coef.final.model.sim$fixef

### merge this to coef
library(abind)
coefArray <- abind(coefArray, coefUnmod)
str(coefArray)
```


# 1.Prediction at current condition
## prepare df for prediction
```{r prep for prediction}
### make another df so as not write over
envDfPred <- envDf15       
envDfPred <- merge(envDfPred, AllHuc10, all.x=TRUE)
envDfPred$fHuc10[ is.na(envDfPred$fHuc10) ] <- n.huc + 1  # +1 to indicate a newly added group

### standardize covariates
envDfPred$stdArea <- (envDfPred$TotDASqKM - mean(envDf15Sampled$TotDASqKM))/sd(envDf15Sampled$TotDASqKM)
envDfPred$stdForest <- (envDfPred$forest - mean(envDf15Sampled$forest))/sd(envDf15Sampled$forest)
envDfPred$stdSlope <- (envDfPred$slope - mean(envDf15Sampled$slope))/sd(envDf15Sampled$slope)
envDfPred$stdSurf_coarse <- (envDfPred$surf_coarse - mean(envDf15Sampled$surf_coarse, na.rm=TRUE))/sd(envDf15Sampled$surf_coarse, na.rm=TRUE) 
# replace NA with mean values for surf_coarse
envDfPred$stdSurf_coarse[is.na(envDfPred$stdSurf_coarse)]  <- mean(envDf15Sampled$surf_coarse, na.rm=TRUE)
envDfPred$stdAirTemp <- (envDfPred$tmin - mean(envDf15Sampled$tmin))/sd(envDf15Sampled$tmin)
envDfPred$stdPrecip <- (envDfPred$precip_annual - mean(envDf15Sampled$precip_annual))/sd(envDf15Sampled$precip_annual)
```

## prediction
```{r prediction, cache=TRUE}
# fill an empty array with prediction
logitPredArray <- array(NA, dim = c(nrow(envDfPred), n.sims))  # empty array
fHuc <- envDfPred$fHuc10   # this is needed for subsetting below

for (i in 1:nrow(envDfPred)){
  for (j in 1:n.sims){
    logitPredArray[i,j] <- coefArray[j,1,fHuc[i]] +   # basin-specific intercept
                           sum(envDfPred[i,56:61]*coefArray[j, 2:n.param, fHuc[i]])   # coef: make sure numbers are right
  }
}

# transform to probabilit scale
predArray <- inv.logit(logitPredArray) 

# mean & 95% CI of occ. prob.
envDfPred$probMean <- apply(predArray,1,mean)
envDfPred$probLow <- apply(predArray,1,quantile,probs=c(0.025),na.rm=TRUE)
envDfPred$probHigh <- apply(predArray,1,quantile,probs=c(0.975),na.rm=TRUE)

write.csv(envDfPred, file="envDfPred1.csv", row.names=FALSE)
```


# 2.Prediction when air temp increases by 4C
## prepare df for prediction
```{r prep for prediction}
### make another df so as not write over
envDfPred2 <- envDf15       
envDfPred2 <- merge(envDfPred2, AllHuc10, all.x=TRUE)
envDfPred2$fHuc10[ is.na(envDfPred2$fHuc10) ] <- n.huc + 1  # +1 to indicate a newly added group

### 4C to current tmin values
envDfPred2$tminC <- (envDfPred2$tmin - 32)*5/9
envDfPred2$tminFuture <- envDfPred2$tminC + 4

### standardize covariates
envDfPred2$stdArea <- (envDfPred2$TotDASqKM - mean(envDf15Sampled$TotDASqKM))/sd(envDf15Sampled$TotDASqKM)
envDfPred2$stdForest <- (envDfPred2$forest - mean(envDf15Sampled$forest))/sd(envDf15Sampled$forest)
envDfPred2$stdSlope <- (envDfPred2$slope - mean(envDf15Sampled$slope))/sd(envDf15Sampled$slope)
envDfPred2$stdSurf_coarse <- (envDfPred2$surf_coarse - mean(envDf15Sampled$surf_coarse, na.rm=TRUE))/sd(envDf15Sampled$surf_coarse, na.rm=TRUE) 
# replace NA with mean values for surf_coarse
envDfPred2$stdSurf_coarse[is.na(envDfPred2$stdSurf_coarse)]  <- mean(envDf15Sampled$surf_coarse, na.rm=TRUE)
# F to C in air temp
envDfPred2$stdAirTemp <- (envDfPred2$tminFuture - mean((envDf15Sampled$tmin-32)*5/9))/(sd(envDf15Sampled$tmin-32)*5/9)
envDfPred2$stdPrecip <- (envDfPred2$precip_annual - mean(envDf15Sampled$precip_annual))/sd(envDf15Sampled$precip_annual)
```

## prediction
```{r prediction, cache=TRUE}
# fill an empty array with prediction
logitPredArray2 <- array(NA, dim = c(nrow(envDfPred2), n.sims))  # empty array
fHuc <- envDfPred2$fHuc10   # this is needed for subsetting below

for (i in 1:nrow(envDfPred2)){
  for (j in 1:n.sims){
    logitPredArray2[i,j] <- coefArray[j,1,fHuc[i]] +   # basin-specific intercept
                            sum(envDfPred2[i,58:63]*coefArray[j, 2:n.param, fHuc[i]])   # coef: make sure numbers are right
  }
}

# transform to probability scale
predArray2 <- inv.logit(logitPredArray2) 

# mean & 95% CI of occ. prob.
envDfPred2$probMean <- apply(predArray2,1,mean)
envDfPred2$probLow <- apply(predArray2,1,quantile,probs=c(0.025),na.rm=TRUE)
envDfPred2$probHigh <- apply(predArray2,1,quantile,probs=c(0.975),na.rm=TRUE)

write.csv(envDfPred2, file="envDfPred2.csv", row.names=FALSE)
```


# 3.Prediction when precipitation decreases by 25%
## prepare df for prediction
```{r prep for prediction}
### make another df so as not write over
envDfPred3 <- envDf15       
envDfPred3 <- merge(envDfPred3, AllHuc10, all.x=TRUE)
envDfPred3$fHuc10[ is.na(envDfPred3$fHuc10) ] <- n.huc + 1  # +1 to indicate a newly added group

### 75% of current precipitation 
envDfPred3$precipFuture <- envDfPred3$precip_annual*0.75 

### standardize covariates
envDfPred3$stdArea <- (envDfPred3$TotDASqKM - mean(envDf15Sampled$TotDASqKM))/sd(envDf15Sampled$TotDASqKM)
envDfPred3$stdForest <- (envDfPred3$forest - mean(envDf15Sampled$forest))/sd(envDf15Sampled$forest)
envDfPred3$stdSlope <- (envDfPred3$slope - mean(envDf15Sampled$slope))/sd(envDf15Sampled$slope)
envDfPred3$stdSurf_coarse <- (envDfPred3$surf_coarse - mean(envDf15Sampled$surf_coarse, na.rm=TRUE))/sd(envDf15Sampled$surf_coarse, na.rm=TRUE) 
# replace NA with mean values for surf_coarse
envDfPred3$stdSurf_coarse[is.na(envDfPred3$stdSurf_coarse)]  <- mean(envDf15Sampled$surf_coarse, na.rm=TRUE)
envDfPred3$stdAirTemp <- (envDfPred3$tmin - mean(envDf15Sampled$tmin))/sd(envDf15Sampled$tmin)
envDfPred3$stdPrecip <- (envDfPred3$precipFuture - mean(envDf15Sampled$precip_annual))/sd(envDf15Sampled$precip_annual)
```

## prediction
```{r prediction, cache=TRUE}
# fill an empty array with prediction
logitPredArray3 <- array(NA, dim = c(nrow(envDfPred3), n.sims))  # empty array
fHuc <- envDfPred3$fHuc10   # this is needed for subsetting below

for (i in 1:nrow(envDfPred3)){
  for (j in 1:n.sims){
    logitPredArray3[i,j] <- coefArray[j,1,fHuc[i]] +   # basin-specific intercept
                            sum(envDfPred3[i,57:62]*coefArray[j, 2:n.param, fHuc[i]])   # coef: make sure numbers are right
  }
}

# transform to probability scale
predArray3 <- inv.logit(logitPredArray3) 

# mean & 95% CI of occ. prob.
envDfPred3$probMean <- apply(predArray3,1,mean)
envDfPred3$probLow <- apply(predArray3,1,quantile,probs=c(0.025),na.rm=TRUE)
envDfPred3$probHigh <- apply(predArray3,1,quantile,probs=c(0.975),na.rm=TRUE)

write.csv(envDfPred3, file="envDfPred3.csv", row.names=FALSE)
```


# 4.Prediction when forest cover decreases by 25%
## prepare df for prediction
```{r prep for prediction}
### make another df so as not write over
envDfPred4 <- envDf15       
envDfPred4 <- merge(envDfPred4, AllHuc10, all.x=TRUE)
envDfPred4$fHuc10[ is.na(envDfPred4$fHuc10) ] <- n.huc + 1  # +1 to indicate a newly added group

### 75% of current forest 
envDfPred4$forestFuture <- envDfPred4$forest*0.75 

### standardize covariates
envDfPred4$stdArea <- (envDfPred4$TotDASqKM - mean(envDf15Sampled$TotDASqKM))/sd(envDf15Sampled$TotDASqKM)
envDfPred4$stdForest <- (envDfPred4$forestFuture - mean(envDf15Sampled$forest))/sd(envDf15Sampled$forest)
envDfPred4$stdSlope <- (envDfPred4$slope - mean(envDf15Sampled$slope))/sd(envDf15Sampled$slope)
envDfPred4$stdSurf_coarse <- (envDfPred4$surf_coarse - mean(envDf15Sampled$surf_coarse, na.rm=TRUE))/sd(envDf15Sampled$surf_coarse, na.rm=TRUE) 
# replace NA with mean values for surf_coarse
envDfPred4$stdSurf_coarse[is.na(envDfPred4$stdSurf_coarse)]  <- mean(envDf15Sampled$surf_coarse, na.rm=TRUE)
envDfPred4$stdAirTemp <- (envDfPred4$tmin - mean(envDf15Sampled$tmin))/sd(envDf15Sampled$tmin)
envDfPred4$stdPrecip <- (envDfPred4$precip_annual - mean(envDf15Sampled$precip_annual))/sd(envDf15Sampled$precip_annual)
```

## prediction
```{r prediction, cache=TRUE}
# fill an empty array with prediction
logitPredArray4 <- array(NA, dim = c(nrow(envDfPred4), n.sims))  # empty array
fHuc <- envDfPred4$fHuc10   # this is needed for subsetting below

for (i in 1:nrow(envDfPred4)){
  for (j in 1:n.sims){
    logitPredArray4[i,j] <- coefArray[j,1,fHuc[i]] +   # basin-specific intercept
                            sum(envDfPred4[i,57:62]*coefArray[j, 2:n.param, fHuc[i]])   # coef: make sure numbers are right
  }
}

# transform to probability scale
predArray4 <- inv.logit(logitPredArray4) 

# mean & 95% CI of occ. prob.
envDfPred4$probMean <- apply(predArray4,1,mean)
envDfPred4$probLow <- apply(predArray4,1,quantile,probs=c(0.025),na.rm=TRUE)
envDfPred4$probHigh <- apply(predArray4,1,quantile,probs=c(0.975),na.rm=TRUE)

write.csv(envDfPred4, file="envDfPred4.csv", row.names=FALSE)
```

## Combine the mean response from the four scenarios
```{r}
envDfPredMeanRes <- cbind(envDfPred[,c("FEATUREID","TotDASqKM","probMean")], 
                          envDfPred2$probMean, envDfPred3$probMean, envDfPred4$probMean)
names(envDfPredMeanRes)[names(envDfPredMeanRes)=="probMean"] <- "probMean1"
names(envDfPredMeanRes)[names(envDfPredMeanRes)=="envDfPred2$probMean"] <- "probMean2"
names(envDfPredMeanRes)[names(envDfPredMeanRes)=="envDfPred3$probMean"] <- "probMean3"
names(envDfPredMeanRes)[names(envDfPredMeanRes)=="envDfPred4$probMean"] <- "probMean4"

# select catchment <= 15 km2, so as to merge with > 15 km2 analysis
envDfPredMeanResUnder15sqkm <- subset(envDfPredMeanRes, TotDASqKM <= 15)

write.csv(envDfPredMeanRes, file="envDfPredMeanResUnder15sqkm.csv", row.names=FALSE)
```

### Wenger et al. (2013) Global Change Biology
```{r Wenger et al. 2013}
nreps <- 5

for(j in 1:nreps){
  predtable <- matrix(rep(0,nreps*nrow(data.pred.std)),ncol=nreps) # create empty matrix for preds
  randparm <- mvrnorm(n=10,mu=fixef(final.model),Sigma=vcov(final.model))
  mm <- model.matrix(terms(final.model),data.fit.std)
  ### use known random effect value where available
  randeff <- ranef(final.model)$fHuc10[match(as.character(data.pred.std$fHuc10),row.names(ranef(final.model)$fHuc10[1])),1]
  ### For other hucs, generate random value and apply it to each record
  randhuc8 <- rnorm(length(unique(data.pred.std$fHuc10)),0,as.numeric(VarCorr(final.model)[1])^.5)
  names(randhuc8) <- as.character(unique(data.pred.std$fHuc10))
  randeff[is.na(randeff)] <- randhuc8[match(as.character(data.pred.std$fHuc10),names(randhuc8))]
  predtable[,j] <- round(plogis(0,location=((mm%*%randparm)+randeff),scale=1,lower.tail=F),3)
}

mySumm <- function(.) { s <- sigma(.)
c(beta =getME(., "beta"), sigma = s, sig01 = unname(s * getME(., "theta"))) }

shinobu <- bootMer(final.model, mySumm, nsim=2)
```


# For Ana
```{r website for Ana}
## mean & 95% CI occ. prob.
#webOccProb <- envDfPred[ , c("FEATUREID","HUC_10","probMean","probLow","probHigh")]

## fit data set
#env.fit.df <- data.fit3

## prediction data set 
env.pred.df <- data.pred

## prediction values
occ.pred <- 

## coef
coef.array <- coefArray

## HUC10 ID
Huc10.df <- AllHuc10

## Mean & SD for standardization
value = c("mean","sd")
log.area = c(mean(data.fit3$log.area), sd(data.fit3$log.area))
asin.forest = c(mean(data.fit3$asin.forest), sd(data.fit3$asin.forest))
sqrt.slope = c(mean(data.fit3$sqrt.slope), sd(data.fit3$sqrt.slope))
precip = c(mean(data.fit3$precip), sd(data.fit3$precip))
tmin = c(mean(data.fit3$tmin), sd(data.fit3$tmin))
drainC = c(mean(data.fit3$drainC, na.rm=TRUE), sd(data.fit3$drainC, na.rm=TRUE))
standardized.df <- data.frame(value, log.area, asin.forest, sqrt.slope, precip, tmin, drainC) 
```

```{r deleting unnecessary objects, eval=FALSE}
## packaging the above objects for Ana
library(gdata)
keep('env.pred.df', 'coef.array', 'Huc10.df', 'standardized.df', sure=TRUE)
```
